[[{"l":"AutoAttend Documentation","p":["Welcome to the AutoAttend documentation. AutoAttend is a production-grade facial recognition system designed for educational institutions to streamline attendance tracking."]},{"l":"Documentation Sections","p":["Face Recognition Settings- Detailed configuration options for the face recognition system, including models, thresholds, and tracking parameters","Cost Calculator- Interactive tool to estimate deployment costs based on hardware requirements, model selection, and performance settings","Registration System- Detailed documentation for the student registration process, API endpoints, and face embedding technical details","System Architecture- Comprehensive overview of the AutoAttend architecture, Kubernetes configuration, deployment/installation process and operational aspects"]},{"l":"About AutoAttend","p":["AutoAttend uses advanced facial recognition technology to capture and process student attendance data automatically. The system features real-time face detection, vector-based storage, and secure authentication, all deployed on a Kubernetes infrastructure for scalability and reliability.","Developed by Soroban Labs"]}],[{"l":"Face Recognition Settings","p":["This document provides an in-depth overview of the customizable face recognition configuration options available in the AutoAttend system. These settings enable precise control over detection accuracy, recognition performance, and resource utilization to balance system requirements."]},{"l":"Settings Overview","p":["The face recognition system provides extensive configuration options across four main categories:","Model Selection- Choose between different detection and recognition models","Detection Parameters- Fine-tune confidence thresholds and frame processing","Performance Optimization- Balance processing speed and resource utilization","Tracking Configuration- Configure advanced tracking parameters for smooth recognition"]},{"l":"Configuration Interface","p":["The configuration interface is organized into tabbed sections for easy navigation:","Face Recognition Settings Interface The settings dialog provides a comprehensive interface for configuring all face recognition parameters."]},{"l":"Model Settings","p":["AutoAttend supports multiple face detection and recognition models to accommodate various performance and accuracy requirements."]},{"l":"Detection Models","p":["25.5 MB","46.2 MB","6.5 MB","7.8 MB","89.7 MB","Accuracy","Balanced","Fastest","File Size","General-purpose","Good","High","High accuracy requirements","Highest","Lower","Mobile, edge devices","Model","Moderate","Research, critical applications","Resource-constrained systems","Slower","Slowest","Speed","Use Case","Very Fast","YOLOv10n-face","YOLOv11l-face","YOLOv8l-face","YOLOv8m-face","YOLOv8n-face"]},{"l":"Recognition Models","p":["11.4 MB","248.5 MB","96.8 MB","Accuracy","Balanced","Edge devices, high throughput","Fastest","File Size","General-purpose","Good","High security, critical applications","Highest","InsightFace MobileNet","InsightFace R100","InsightFace R50","Lower","Model","Slowest","Speed","Use Case"]},{"l":"Detection Settings","p":["The detection settings control the core behavior of the face detection and recognition pipeline.","Detection Settings Tab Configure detection parameters to balance false positives and false negatives."]},{"l":"Confidence Threshold","p":["The confidence threshold (range: 0.0-1.0) determines how certain the model must be before considering a detection valid.","Higher values(e.g., 0.7-0.9): Fewer false positives, but may miss some faces","Lower values(e.g., 0.3-0.5): Detects more faces, but may include false positives","Recommended: 0.5 for general use"]},{"l":"Face Matcher Threshold","p":["The face matcher threshold (range: 0.0-1.0) controls how similar a detected face must be to a known face for recognition.","Higher values(e.g., 0.7-0.9): More strict matching, lower false acceptance rate","Lower values(e.g., 0.1-0.3): More lenient matching, higher false acceptance rate","Recommended: 0.2 for balanced performance"]},{"l":"Max Frame Size","p":["Maximum frame dimension in pixels for processing. Larger values provide higher detection accuracy but require more processing power.","Range: 320-1920 pixels","Recommended:","640px for real-time processing on standard hardware","1280px for higher accuracy where processing power allows"]},{"l":"Performance Settings","p":["Performance settings allow you to balance processing speed, resource utilization, and detection quality.","Performance Settings Tab Optimize performance settings based on your hardware capabilities and requirements."]},{"l":"Detection Interval","p":["The detection interval determines how often (in frames) full face detection is performed.","Range: 1-10 frames","Lower values(e.g., 1-2): More responsive detection, higher CPU usage","Higher values(e.g., 5-10): More efficient processing, may miss quick movements","Recommended: 3 for general use"]},{"l":"Max FPS","p":["Maximum frames per second to process. Lower values reduce CPU usage.","Range: 1-30 FPS","Lower values(e.g., 5-10): Reduced CPU load, suitable for background processing","Higher values(e.g., 15-30): Smoother experience, better for interactive scenarios","Recommended: 10 for balanced performance"]},{"l":"Face Tracking","p":["When enabled, faces are tracked between frames for smoother recognition and better performance. This setting enables Norfair tracking features.","Enabled: Better performance with smooth tracking between frames","Disabled: Process each frame independently (higher CPU usage)","Recommended: Enabled for most use cases"]},{"l":"Tracker Settings","p":["Advanced settings for fine-tuning the Norfair object tracking system.","Tracker Settings Tab Configure tracking parameters for optimal continuity in face tracking."]},{"l":"Distance Function","p":["Method used to associate detections across frames:","IoU (Intersection over Union): Best for most face tracking scenarios","Euclidean Distance: Alternative for special cases where IoU doesn't perform well","Manhattan Distance: May perform better in certain grid-aligned scenarios","Recommended: IoU for face tracking"]},{"l":"Distance Threshold","p":["Maximum distance for associating detections across frames (range: 0.1-1.0).","Higher values: More lenient association, may confuse different faces","Lower values: Stricter association, may fragment tracks of the same face","Recommended: 0.7 for face tracking"]},{"l":"Initialization Delay","p":["Number of frames before a detection becomes a tracked object (range: 1-10).","Higher values: More stable tracks, less flickering, but slower to initialize","Lower values: Faster tracking initialization, may include some false tracks","Recommended: 3 frames"]},{"l":"Hit Counter Max","p":["Number of frames a track is kept without matching detections (range: 1-30).","Higher values: Tracks persist longer through occlusions","Lower values: Tracks are removed more quickly when faces disappear","Recommended: 12 frames"]},{"l":"Pointwise Hit Counter Max","p":["Fine-grained tracking parameter for individual points in a detection (range: 1-10).","Higher values: More stable tracking of individual facial points","Lower values: Quicker updates to facial point positions","Recommended: 4 for face tracking"]},{"l":"Detection Threshold","p":["Minimum confidence score for detections to be tracked (range: 0.1-1.0).","Higher values: Only high confidence detections are tracked","Lower values: Track lower confidence detections as well","Recommended: 0.5 for balanced performance"]},{"l":"Performance Impact","p":["10-15","25-30","5-8","Balanced","Balanced Configuration:","Configuration","CPU Usage","Detection Accuracy","Detection Interval: 1","Detection Interval: 3","Detection Interval: 6","Excellent","Face Detection: YOLOv11l-face","Face Detection: YOLOv8m-face","Face Detection: YOLOv8n-face","Face Recognition: InsightFace MobileNet","Face Recognition: InsightFace R100","Face Recognition: InsightFace R50","Face Tracking: Enabled","FPS","Good","High","Highest Accuracy","Highest Accuracy Configuration:","Highest Performance","Highest Performance Configuration:","Low","Lower","Max Frame Size: 1280px","Max Frame Size: 480px","Max Frame Size: 640px","Medium","RAM Usage","Recognition Accuracy","The following table illustrates the approximate impact of different configuration combinations on system performance:"]},{"l":"Technical Implementation"},{"l":"Face Detection Process","p":["The AutoAttend system uses a multi-stage processing pipeline for face detection and recognition:"]},{"l":"Settings Integration","p":["The configurable settings directly impact each stage of the processing pipeline:","Model settings determine which neural network models are loaded for detection and recognition","Detection settings control confidence thresholds and input frame sizing","Performance settings manage processing intervals and throughput","Tracker settings fine-tune how faces are tracked between frames"]},{"l":"Best Practices"},{"l":"General Recommendations","p":["Start with the balanced configuration and adjust based on your specific requirements","Monitor CPU and memory usage when changing settings","For high-traffic areas, prioritize performance settings","For security applications, prioritize accuracy settings"]},{"l":"Environment-Specific Recommendations","p":["Environment","Recommended Configuration","Classroom","Balanced, 10-15 FPS","Office Entry","Higher Accuracy, 5-10 FPS","Stadium/Large Venue","Higher Performance, 15-30 FPS","Security Checkpoint","Highest Accuracy, 5-8 FPS"]},{"l":"Troubleshooting"},{"l":"Common Issues","p":["Confidence threshold too high, frame size too small","False recognitions","Frame size too large, model too complex","High CPU usage","Increase detection interval, reduce max FPS","Increase matcher threshold","Increase tracker distance threshold","Issue","Jerky tracking","Low detection interval, high FPS","Lower confidence threshold, increase frame size","Matcher threshold too low","Missed detections","Potential Causes","Reduce frame size, use a smaller model","Slow performance","Solutions","Tracker distance threshold too low"]},{"l":"Performance Monitoring","p":["Monitor the system performance metrics to identify bottlenecks:"]},{"l":"Related Documentation","p":["For comprehensive information about the complete AutoAttend system, please refer to:","AutoAttend Architecture- System architecture and deployment","Registration System- Student enrollment and face embedding capture"]}],[{"l":"AutoAttend Cost Calculator","p":["This document provides a comprehensive overview of the AutoAttend Cost Calculator, a tool designed to help educational institutions estimate the financial implications of deploying the facial recognition attendance system. The calculator offers a transparent view of potential costs based on configuration choices."]},{"l":"Calculator Overview","p":["The AutoAttend Cost Calculator provides estimated costs across four main categories:","Infrastructure Costs- Physical computing resources required for operation","Model Selection- Impact of AI model choices on performance and cost","Processing Settings- Runtime configuration that affects resource utilization","Deployment Scale- Scope and scale of the implementation"]},{"l":"Calculator Interface","p":["The calculator interface is organized into tabbed sections for intuitive navigation:","Cost Calculator Interface The cost calculator provides a comprehensive interface for estimating deployment costs based on various parameters."]},{"l":"Model Selection","p":["The model selection significantly impacts both system performance and overall costs. The calculator allows you to choose between different face detection and recognition models to balance accuracy and resource requirements."]},{"l":"Performance Settings","p":["Performance settings directly impact resource utilization and consequently affect the overall cost of deployment."]},{"l":"Max Frame Size","p":["The maximum frame size determines the resolution of video frames processed for face detection. This can be obtained from the camera specs.","Range: 320-1920 pixels","Lower values(e.g., 320-480px): Lower cost, reduced accuracy","Higher values(e.g., 1280-1920px): Higher cost, improved accuracy","Cost Impact: Higher resolutions increase processing and network costs significantly"]},{"i":"max-fps-frames-per-second","l":"Max FPS (Frames Per Second)","p":["The maximum number of frames processed per second.","Range: 1-30 FPS","Lower values(e.g., 1-5 FPS): Lower cost, suitable for static environments","Higher values(e.g., 15-30 FPS): Higher cost, better for dynamic environments","Cost Impact: Each additional frame increases computational resource requirements linearly"]},{"l":"Face Tracking","p":["Face tracking allows the system to follow faces across frames without full detection on every frame.","Enabled: Slightly higher cost but improves efficiency for continuous monitoring","Disabled: Lower immediate cost but may require more frequent full-frame detections","Cost Impact: Modest increase in computational needs, generally cost-effective for continuous operation"]},{"l":"Deployment Settings","p":["Deployment settings define the scale and scope of your AutoAttend implementation."]},{"l":"Number of Concurrent Streams","p":["The number of camera feeds processed simultaneously.","Range: 1-10 streams","Cost Impact: Linear scaling of processing and resource requirements per stream"]},{"l":"Number of Instances","p":["The number of processing instances deployed (for load balancing or redundancy).","Range: 1-5 instances","Cost Impact: Linear scaling of infrastructure costs per instance"]},{"l":"Usage Pattern","p":["The usage pattern defines when and how often the system will be operational."]},{"l":"Operational Hours","p":["The total time the system will be active, measured in hours per day and days per month.","Hours Range: 1-24 hours per day","Days Range: 1-31 days per month","Optimization Tip: Configuring precise operational hours to match actual class schedules can significantly reduce costs"]},{"l":"Cost Breakdown","p":["The calculator provides a comprehensive cost breakdown across multiple time horizons."]},{"l":"Cost Summary Cards","p":["The calculator provides at-a-glance cost summaries across different time periods:","Cost Summary Cards Quick reference cost cards showing hourly, daily, monthly, and per-stream costs."]},{"l":"Advanced Mode","p":["For institutions with specific pricing information or custom infrastructure requirements, the calculator offers an advanced mode.","Advanced Mode Settings Advanced mode allows customization of all cost parameters for more precise estimates."]},{"l":"Customizable Parameters","p":["Infrastructure costs (CPU, memory, storage, network)","Resource allocation specifics","Custom pricing based on specific cloud providers or on-premises deployments"]},{"l":"Implementation Considerations"},{"l":"Cost Optimization Strategies","p":["The calculator can help identify optimal configurations based on your institution's specific needs:","Scale-Optimized: Maximize number of streams with minimal resource allocation","Performance-Optimized: Prioritize accuracy and responsiveness over cost","Balanced: Find the optimal middle ground between cost and performance"]},{"l":"Deployment Scenarios","p":["Scenario","Typical Configuration","Estimated Monthly Cost Range","Small Institution (1-5 cameras)","YOLOv8n, R50, 640px, 10 FPS","$XXX - $XXX","Medium Institution (5-20 cameras)","YOLO11n, R50, 640px, 10 FPS","Large Institution (20+ cameras)","YOLOv11s, R100, 1280px, 15 FPS"]},{"l":"Best Practices"},{"l":"Getting Accurate Estimates","p":["To get the most accurate cost estimates:","Configure the calculator to match your specific deployment scenario","Consider both peak and average usage patterns","Adjust settings to find the optimal balance between cost and performance","Use advanced mode if you have specific pricing information"]},{"l":"Cost-Effective Configuration","p":["For most educational institutions, we recommend:","Start with a balanced configuration","Monitor actual usage patterns after initial deployment","Adjust parameters based on real-world performance and costs","Consider scaling up during peak periods (beginning/end of semesters)"]},{"l":"Related Documentation","p":["For comprehensive information about the complete AutoAttend system, please refer to:","AutoAttend Architecture- System architecture and deployment","Registration System- Student enrollment and face embedding capture","Face Recognition Settings- Detailed face recognition configuration"]}],[{"l":"AutoAttend Registration System"},{"l":"Overview","p":["The Registration System is a core component of the AutoAttend facial recognition ecosystem, specifically focused on the student enrollment and face embedding capture process. This document details the registration workflow, API endpoints, and technical implementation of the registration features.","Registration Interface The utility offers Google based sign in and quick embedding storage and integrates with the kubernetes architecture.","Database Once the student is done, the admin is able to see all existing records."]},{"l":"Registration Workflow"},{"l":"API Documentation"},{"l":"Embeddings API","p":["Endpoint","Method","Description","/api/embeddings","POST","Store face embeddings","/api/embeddings/{student_id}","GET","Retrieve embeddings for a student","DELETE","Remove a student's embeddings","/api/search","Find matching faces","/api/students","List all registered students"]},{"i":"example-storing-embeddings","l":"Example: Storing Embeddings"},{"i":"example-searching-for-a-match","l":"Example: Searching for a Match"},{"l":"Technical Details"},{"l":"Face Embeddings","p":["The registration system generates 128-dimensional face embeddings using face-api.js, which is powered by TensorFlow.js. These embeddings capture the unique facial features of individuals and allow for efficient similarity searches."]},{"l":"Milvus Configuration","p":["The registration system uses a specialized HNSW (Hierarchical Navigable Small World) index for fast approximate nearest neighbor search, which is ideal for face recognition:"]},{"l":"Database Schema","p":["The face embeddings collection has the following structure:","Field","Type","Description","id","INT64","Primary key (auto-generated)","student_id","VARCHAR","Unique identifier for the student","embedding","FLOAT_VECTOR[128]","Face embedding vector","timestamp","Unix timestamp of registration"]},{"l":"Troubleshooting"},{"l":"Common Registration Issues","p":["Models not loading: Ensure models are downloaded to the public/models directory","Webcam not working: Check browser permissions and ensure your device has a working camera","Face detection issues: Improve lighting and position face clearly in the frame","Registration failures: Verify that the student ID is unique and properly formatted"]},{"l":"Related Documentation","p":["For comprehensive information about the AutoAttend system architecture, deployment, and Kubernetes configuration, please refer to the AutoAttend Architecture documentation."]}],[{"l":"AutoAttend Architecture","p":["This repository provides production-ready Kubernetes configurations for deploying AutoAttend, a scalable facial recognition attendance tracking system. AutoAttend uses Milvus vector database for efficient face matching and supports real-time attendance monitoring via web interface.","Home Page","Logs Streams","The system shows logs of detected features, allows downloading them and more for each stream. The system also allows turning on/off streams."]},{"l":"System Architecture","p":["AutoAttend is architected as a cloud-native application following microservices patterns and best practices for Kubernetes deployments."]},{"l":"Key Components"},{"l":"Namespaces","p":["The system uses Kubernetes namespaces to isolate components:","Namespace","Description","frontend","User interface components","backend","API services and application logic","data","Databases and message brokers","ml-inference","ML processing services"]},{"l":"Component Breakdown","p":["Asynchronous task processing","Backend API","Caching and Celery result backend","Celery","Celery Workers","Component","Custom","Face detection and recognition processing","FastAPI","FastAPI/WebSockets","Frontend","Message brokering for task queue","Milvus","Milvus Vector DB","ML Inference","Next.js","Purpose","RabbitMQ","Real-time communication channel","Redis","RESTful API endpoints for system interaction","Storage and querying of face embeddings","Technology","User interface for attendance tracking","WebSocket"]},{"l":"Prerequisites","p":["Before deploying, ensure you have:","Kubernetes cluster (1.18+)","kubectl CLI tool configured to connect to your cluster","Docker and container registry access","Domain name (for production deployment)","TLS certificates (for production deployment)"]},{"l":"Configuration"},{"l":"Environment Variables","p":["API subdomain","API_DOMAIN","api.attendance.example.com","attendance.example.com","Description","Docker registry password","Docker registry username","DOCKER_PASSWORD","DOCKER_USERNAME","DOMAIN","Example","Key configuration parameters:","password","Primary domain name","RabbitMQ password","RabbitMQ username","RABBITMQ_PASSWORD","RABBITMQ_USER","Redis password","REDIS_PASSWORD","strong-random-password","The system is configured using environment variables. Copy the .env.example file to .env for production deployment:","user","username","Variable","WebSocket subdomain","WS_DOMAIN","ws.attendance.example.com"]},{"l":"Deployment"},{"l":"Production Deployment","p":["For production environments:","This script:","Creates production-grade configurations","Sets up TLS/SSL with cert-manager","Configures ingress rules","Applies resource limits and auto-scaling","Sets up network policies for security"]},{"l":"Security Features","p":["The deployment implements several security best practices:","Namespace isolation with network policies","Non-root container execution","Resource quotas and limits","TLS/SSL encryption for all traffic","Secret management for sensitive information","RBAC for service accounts","Network policies restricting pod-to-pod communication"]},{"l":"Network Policies","p":["Network policies enforce strict rules for pod-to-pod communication, implementing the principle of least privilege to minimize the attack surface."]},{"l":"Detailed Network Policy Configuration","p":["Each namespace has one or more network policies that restrict pod-to-pod communication:"]},{"l":"Deployment Process","p":["The deployment process follows a structured approach to ensure consistency and reliability in both development and production environments."]},{"l":"Maintenance Operations"},{"l":"Scaling and Reliability","p":["The system implements both manual scaling and Horizontal Pod Autoscaling (HPA) for dynamic resource adjustment based on workload."]},{"i":"cicd-and-update-process","l":"CI/CD and Update Process","p":["The system implements a robust CI/CD pipeline and structured update process to ensure reliable deployments."]},{"l":"Monitoring and Observability","p":["The system implements comprehensive monitoring and observability for real-time system health tracking."]},{"l":"Health Checks and Monitoring Commands"},{"l":"Face Recognition System","p":["The system uses advanced facial recognition algorithms to track attendance:","Face Detection: Detects and isolates faces in camera feeds","Face Embedding: Converts facial features into vector representations","Vector Matching: Compares embeddings against stored profiles in Milvus","Attendance Recording: Records matches with timestamps in the database"]},{"l":"Milvus Configuration","p":["The Milvus vector database is configured for efficient face embedding storage and retrieval:"]},{"l":"Troubleshooting"},{"l":"Common Issues","p":["Issue","Solution","Pods in CrashLoopBackOff","Check logs: kubectl logs -n namespace pod-name","Database connection errors","Verify secrets and check data service health","Image pull errors","Check Docker registry credentials","Ingress not working","Verify ingress controller and DNS configuration"]},{"l":"Debugging Commands"},{"l":"Best Practices","p":["Resource Management: Set appropriate requests and limits for all containers","Liveness/Readiness Probes: Implement for all services for better auto-healing","StatefulSets: Use for stateful services like databases","ConfigMaps/Secrets: Separate configuration from code","Rolling Updates: Use for zero-downtime deployments","Namespace Isolation: Keep components separated for better security"]},{"l":"Additional Resources","p":["Kubernetes Documentation","Milvus Documentation","FastAPI Documentation","Next.js Documentation"]}]]